---
# http://prashplus.blogspot.com/2018/01/ceph-single-node-setup-ubuntu.html
# tasks file for ceph

- name: "Install CEPH server"
  debug:
    msg: "Install CEPH server"

#- name: "Add CEPH GPG key"
#  apt_key:
#    url: "https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc"
    #validate_certs: false

#- name: "Add CEPH APT repository"
#  apt_repository:
#    repo: "deb http://ceph.com/archive/debian-dumpling/ wheezy main"

- name: Install CEPH
  package:
    name: "{{item}}"
  with_items:
    - ceph-deploy
    - ceph-mds
    - ceph-common
    - dnsmasq


# http://prashplus.blogspot.com/2018/01/ceph-single-node-setup-ubuntu.html
# sudo useradd -m -s /bin/bash ceph-deploy
# sudo passwd ceph-deploy
- name: "Add ceph-deploy user"
  user:
    name: ceph-deploy
    shell: /bin/bash
    #groups: ceph-deploy
    password: lol

# sudo echo "ceph-deploy ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/ceph-deploy
# sudo chmod 0440 /etc/sudoers.d/ceph-deploy
- name: "Add ceph-deploy as sudoer"
  lineinfile:
    path: /etc/sudoers.d/ceph-deploy
    line: "ceph-deploy ALL = (root) NOPASSWD:ALL"
    create: yes
    mode: 0440

# sudo su - ceph-deploy

# /etc/ssh/sshd_config - PasswordAuthentication yes - service ssh restart
- name: "Change ssh config"
  lineinfile:
    path: /etc/ssh/sshd_config
    regexp: 'PasswordAuthentication'
    line: "PasswordAuthentication yes"
  register: ssh_config

- name: Restart ssh service
  service:
    name: ssh
    state: restarted
  when: ssh_config.changed

# /etc/hosts - 192.168.99.50   ubuntu-data - 192.168.33.50   ubuntu-data
- name: "Update /etc/hosts"
  lineinfile:
    path: /etc/hosts
    line: "{{ item }}"
  with_items:
    - "192.168.99.50   ubuntu-data"
    - "192.168.33.50   ubuntu-data"

# mkdir /home/ceph-deploy/my-cluster
- name: "Create directories"
  file:
    path: "{{ item }}"
    state: directory
    mode: 0755
    owner: ceph-deploy
    group: ceph-deploy
  with_items:
    - "/home/ceph-deploy/my-cluster"
    - "/home/ceph-deploy/.ssh"

# /home/ceph-deploy/.ssh/config
- name: "add ssh config"
  template:
    src: ssh-config.tpl
    dest: /home/ceph-deploy/.ssh/config
    owner: ceph-deploy
    group: ceph-deploy

# ssh-keygen
- name: "ssh-keygen"
  command: ssh-keygen -q -t rsa -f /home/ceph-deploy/.ssh/id_rsa  -C '' -N ''
  args:
    creates: /home/ceph-deploy/.ssh/id_rsa.pub
  become_user: ceph-deploy

# ssh-copy-id ceph-deploy@ubuntu-data
- name: "Set authorized key taken from file"
  shell: touch /home/ceph-deploy/.ssh/authorized_keys && cat /home/ceph-deploy/.ssh/id_rsa.pub > /home/ceph-deploy/.ssh/authorized_keys
  args:
    creates: /home/ceph-deploy/.ssh/authorized_keys
  become_user: ceph-deploy

# ceph-deploy new ubuntu-data
- name: "deploy new cluster"
  shell: cd /home/ceph-deploy/my-cluster && ceph-deploy new ubuntu-data
  args:
    creates: /home/ceph-deploy/my-cluster/ceph.mon.keyring
  become_user: ceph-deploy

# ceph.conf - osd pool default size = 2 - osd crush chooseleaf type = 0
- name: "ceph.conf"
  lineinfile:
    path: /home/ceph-deploy/my-cluster/ceph.conf
    line: "{{ item }}"
  with_items:
    - "osd pool default size = 2"
    - "osd crush chooseleaf type = 0"

# ceph-deploy install ubuntu-data
- name: "ceph-deploy install ubuntu-data"
  shell: cd /home/ceph-deploy/my-cluster && ceph-deploy install ubuntu-data && touch /home/ceph-deploy/my-cluster/install-ubuntu-data.log
  args:
    creates: /home/ceph-deploy/my-cluster/install-ubuntu-data.log
  become_user: ceph-deploy

# ceph-deploy mon create-initial
- name: "ceph-deploy mon create-initial"
  shell: cd /home/ceph-deploy/my-cluster && ceph-deploy mon create-initial && touch /home/ceph-deploy/my-cluster/mon-create-initial.log
  args:
    creates: /home/ceph-deploy/my-cluster/mon-create-initial.log
  become_user: ceph-deploy

# ceph-deploy admin ubuntu-data
- name: "ceph-deploy admin ubuntu-data"
  shell: cd /home/ceph-deploy/my-cluster && ceph-deploy admin ubuntu-data && touch /home/ceph-deploy/my-cluster/admin-ubuntu-data.log
  args:
    creates: /home/ceph-deploy/my-cluster/admin-ubuntu-data.log
  become_user: ceph-deploy

# ceph-deploy mgr create ubuntu-data
- name: "ceph-deploy mgr create ubuntu-data"
  shell: cd /home/ceph-deploy/my-cluster && ceph-deploy mgr create ubuntu-data && touch /home/ceph-deploy/my-cluster/mgr-create-ubuntu-data.log
  args:
    creates: /home/ceph-deploy/my-cluster/mgr-create-ubuntu-data.log
  become_user: ceph-deploy

# sudo ceph-disk zap /dev/sdc
# sudo ceph-disk zap /dev/sdd
- name: "ceph-disk zap"
  shell: "ceph-disk zap /dev/{{ item }} && touch /home/ceph-deploy/my-cluster/ceph-disk-zap-{{ item }}.log"
  args:
    creates: "/home/ceph-deploy/my-cluster/ceph-disk-zap-{{ item }}.log"
  with_items:
    - sdc
    - sdd

# ceph-deploy osd prepare ubuntu-data:sdc
# ceph-deploy osd prepare ubuntu-data:sdd
- name: "ceph-deploy osd prepare"
  shell: "cd /home/ceph-deploy/my-cluster && ceph-deploy osd prepare ubuntu-data:{{ item }} && touch /home/ceph-deploy/my-cluster/osd-prepare-{{ item }}.log"
  args:
    creates: /home/ceph-deploy/my-cluster/osd-prepare-{{ item }}.log
  with_items:
    - sdc
    - sdd
  ignore_errors: yes
  #no_log: True

# ceph-deploy osd activate ubuntu-data:/dev/sdc1
# ceph-deploy osd activate ubuntu-data:/dev/sdd1
- name: "ceph-deploy osd activate"
  shell: "cd /home/ceph-deploy/my-cluster && ceph-deploy osd activate ubuntu-data:/dev/{{ item }} && touch /home/ceph-deploy/my-cluster/osd-activate-{{ item }}.log"
  args:
    creates: /home/ceph-deploy/my-cluster/osd-activate-{{ item }}.log
  with_items:
    - sdc1
    - sdd1
  ignore_errors: yes
  #no_log: True

# ceph-deploy osd create ubuntu-data:/dev/sdc
# ceph-deploy osd create ubuntu-data:/dev/sdd

- name: "Check CEPH health"
  shell: "ceph -s | grep health | tr -d '[:blank:]' | tr -d 'health:'"
  register: ceph_healph

- debug:
    msg: "{{ ceph_healph.stdout }}"

- name: "Check CEPH usage"
  shell: "ceph -s | grep usage | tr -d '[:blank:]' | tr -d 'usage:'"
  register: ceph_usage

- debug:
    msg: "{{ ceph_usage.stdout }}"


# CEPH FS

# sudo ceph osd pool create cephfs_data 64
# sudo ceph osd pool create cephfs_metadata 64

# sudo ceph fs new cephfs cephfs_metadata cephfs_data

# sudo mkdir /mnt/cephfs

# cat ~/my-cluster/ceph.client.admin.keyring

# sudo mount.ceph ubuntu-data:6789:/ /mnt/cephfs/ -o name=admin,secret=AQA27jFcI44XDRAA6j5tnSsXKl1JMvpQilswzw==

